{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPbYcxJ1WLxw2HS42lbdQoj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SenecaX/colab-notebooks/blob/main/simple_chat_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI5PDljXlDpq",
        "outputId": "4c71a264-1aca-4314-8855-73db92d3509f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.3.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.6.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.1)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUBrxxxglrct",
        "outputId": "be900559-5286-4ce7-8bf0-b2adbc5353ad"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = [\n",
        "  {\n",
        "    \"question\": \"What's the primary objective of the Idea Validation Platform?\",\n",
        "    \"answer\": \"The key aim is to give budding entrepreneurs and idea generators the tools to gauge the potential of their startup ideas using machine learning and analytics.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What fields are required on the Idea Validation Page?\",\n",
        "    \"answer\": \"You'll need to fill in a text field for 'Idea Name', choose from a 'Market' dropdown, select a 'Competition Level' via radio buttons, input 'Target Audience' in text, and provide a numeric 'Estimated Budget'.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What can a user do on the Results Page?\",\n",
        "    \"answer\": \"Users can either click 'Back' to modify data on the Idea Validation Page or opt to 'Save' their results (optional).\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What programming languages and frameworks are deployed in the frontend?\",\n",
        "    \"answer\": \"The frontend uses React framework and is written in TypeScript, HTML, and CSS.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"How is security maintained?\",\n",
        "    \"answer\": \"Security features include HTTPS protocols, protective measures against SQL injection and XSS, and an optional JWT authentication method.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"List the API endpoints.\",\n",
        "    \"answer\": \"API routes include /api/validate, /api/results, and /api/recommendations.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What testing methodologies are used?\",\n",
        "    \"answer\": \"Jest is employed for unit tests, while API testing utilizes tools similar to Postman.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What's the backend setup?\",\n",
        "    \"answer\": \"The backend is built on Express.js, coded in TypeScript, and uses MongoDB for data storage.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What can be accessed via the User Profile?\",\n",
        "    \"answer\": \"Users can check previous validation outcomes and tweak their personal settings.\"\n",
        "  },\n",
        "  {\n",
        "    \"question\": \"What's the deployment strategy?\",\n",
        "    \"answer\": \"The deployment process involves a CI/CD mechanism via GitHub Actions and hosting on cloud services such as AWS or Heroku.\"\n",
        "  }\n",
        "]\n"
      ],
      "metadata": {
        "id": "NMWYCbToltBI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hA5u2lrrmNM7",
        "outputId": "e064f511-13f3-488a-e0be-33252a45de8d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to preprocess text\n",
        "def preprocess(text):\n",
        "    # Tokenize text\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Preprocess questions and answers\n",
        "for item in dataset:\n",
        "    item['question'] = preprocess(item['question'])\n",
        "    item['answer'] = preprocess(item['answer'])\n"
      ],
      "metadata": {
        "id": "JvTTOo4KmFSW"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TF-IDF vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Fit and transform the questions\n",
        "questions = [item['question'] for item in dataset]\n",
        "question_vectors = tfidf_vectorizer.fit_transform(questions)\n"
      ],
      "metadata": {
        "id": "RRcPK7LQmUr8"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "user_query = \"are there any testing technologies used?\"\n",
        "\n",
        "# Preprocess the user query\n",
        "user_query = preprocess(user_query)\n",
        "\n",
        "# Vectorize the user query\n",
        "user_query_vector = tfidf_vectorizer.transform([user_query])\n",
        "\n",
        "# Calculate cosine similarities between the user query and dataset questions\n",
        "similarities = cosine_similarity(user_query_vector, question_vectors)\n",
        "\n",
        "# Find the index of the most similar question\n",
        "most_similar_index = similarities.argmax()\n",
        "\n",
        "# Get the corresponding answer\n",
        "response = dataset[most_similar_index]['answer']\n",
        "\n",
        "print(response)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XzqqbB5nmXxE",
        "outputId": "f3440634-73b8-4080-bcec-3c9e250e3830"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jest employed unit tests , api testing utilizes tools similar postman .\n"
          ]
        }
      ]
    }
  ]
}